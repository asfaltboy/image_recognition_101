{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image recognition 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_BASE_PATH = os.path.join('media', 'input')\n",
    "GREG_IMAGE_PATH = os.path.join(INPUT_BASE_PATH, 'greg.png')\n",
    "SUDOKU_IMAGE_PATH = os.path.join(INPUT_BASE_PATH, 'sudoku.jpg')\n",
    "MEMESFUNNY_IMAGE_PATH = os.path.join(INPUT_BASE_PATH, 'memesfunny.png')\n",
    "LOGO_IMAGE_PATH = os.path.join(INPUT_BASE_PATH, 'logo.png')\n",
    "LOGO_MULTIPLE_SCREENSHOT_IMAGE_PATH = os.path.join(INPUT_BASE_PATH, 'logo_multiple_screenshot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image):\n",
    "    %matplotlib notebook\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.rcParams['figure.dpi'] = 150\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    \n",
    "def display_greyscale_image(image):\n",
    "    %matplotlib notebook\n",
    "    plt.rcParams['figure.dpi'] = 150\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap='gray', vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_image = cv2.imread(GREG_IMAGE_PATH, cv2.IMREAD_COLOR)\n",
    "color_image.shape\n",
    "display_image(color_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_image[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "OpenCV constants for the exercise:\n",
    "+ IMREAD_COLOR\n",
    "+ IMREAD_GRAYSCALE\n",
    "+ IMREAD_UNCHANGED\n",
    "+ IMREAD_REDUCED_GRAYSCALE_2\n",
    "+ IMREAD_REDUCED_COLOR_2\n",
    "+ IMREAD_REDUCED_GRAYSCALE_4\n",
    "+ IMREAD_REDUCED_COLOR_4\n",
    "+ IMREAD_REDUCED_GRAYSCALE_8\n",
    "+ IMREAD_REDUCED_COLOR_8\n",
    "\n",
    "Try to read greg image with different input flags listed above.\n",
    "Take a look at the results using next cell.\n",
    "Don't forget to check what is the shape of your newly read image and what is the value for specific pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "display_image(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(GREG_IMAGE_PATH, cv2.IMREAD_COLOR)\n",
    "blurred_image_10 = cv2.blur(image, (10, 10))\n",
    "display_image(blurred_image_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "Try out different kernel values for `blur` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(GREG_IMAGE_PATH, cv2.IMREAD_GRAYSCALE)\n",
    "laplacian = cv2.Laplacian(image, cv2.CV_64F)\n",
    "enhanced_laplacian = laplacian.copy()\n",
    "enhanced_laplacian[enhanced_laplacian > 10] = 255\n",
    "display_greyscale_image(enhanced_laplacian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "In the next cell read image of the sudoku puzzle. Try to detect edges using Laplacian method.\n",
    "\n",
    "Compare it with Sobel operator using following function:\n",
    "\n",
    "`cv2.Sobel(input_image, cv2.CV_64F, 1, 0, ksize=ksize_parameter)`\n",
    "\n",
    "Play around with `ksize` parameter for better results.\n",
    "\n",
    "**Remember that `ksize` parameter can only be odd number from range -1 to 31**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudoku_image = cv2.imread(SUDOKU_IMAGE_PATH, cv2.IMREAD_GRAYSCALE)\n",
    "# your code goes here\n",
    "display_greyscale_image(edge_detection_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keypoints detection using SIFT algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(GREG_IMAGE_PATH, cv2.IMREAD_COLOR)\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "keypoints = sift.detect(image, None)\n",
    "image_with_keypoints = cv2.drawKeypoints(image, keypoints, outImage=np.array([]), flags=cv2.DRAW_MATCHES_FLAGS_DEFAULT)\n",
    "display_image(image_with_keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keypoints detection using ORB algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orb = cv2.ORB_create()\n",
    "keypoints = orb.detect(image, None)\n",
    "image_with_keypoints = cv2.drawKeypoints(image, keypoints, outImage=np.array([]), flags=cv2.DRAW_MATCHES_FLAGS_DEFAULT)\n",
    "display_image(image_with_keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "\n",
    "Flags for drawKeypoints:\n",
    "+ DRAW_MATCHES_FLAGS_DEFAULT *only center point*\n",
    "+ DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS *center point with keypoint size and orientation*\n",
    "+ DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS *single keypoints will not be drawn*\n",
    "\n",
    "Observe changes in result image using different flags for drawKeypoints function.\n",
    "\n",
    "Try to analyze what are the characteristic features of keypoints that were selected by algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "display_image(image_with_keypoints_with_different_flags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "greg_image = cv2.imread(GREG_IMAGE_PATH)\n",
    "memesfunny_image = cv2.imread(MEMESFUNNY_IMAGE_PATH)\n",
    "\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "greg_keypoints, greg_descriptors = sift.detectAndCompute(greg_image, None)\n",
    "memesfunny_keypoints, memesfunny_descriptors = sift.detectAndCompute(memesfunny_image, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5\n",
    "Display keypoints found for memesfunny image using `drawKeypoints` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "display_image(memesfunny_image_with_keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = cv2.BFMatcher()\n",
    "matches = bf.match(greg_descriptors, memesfunny_descriptors)\n",
    "\n",
    "first_match = matches[0]\n",
    "\n",
    "print(first_match.distance)\n",
    "print(first_match.queryIdx)\n",
    "print(first_match.trainIdx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "matches_image = cv2.drawMatches(greg_image, greg_keypoints, memesfunny_image, memesfunny_keypoints, matches[:10], outImg=np.array([]), flags=2)\n",
    "display_image(matches_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knn matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = bf.knnMatch(greg_descriptors, memesfunny_descriptors, k=2)\n",
    "match, nearest_neighbour_match = matches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6\n",
    "Display distance attribute for both `match` and `nearest_neighbour_match`.\n",
    "\n",
    "Compare those values. What do they tell us about specific match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowe's ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowe_ratio = 0.75\n",
    "\n",
    "good_matches = []\n",
    "for best_match, second_best_match in matches:\n",
    "    if best_match.distance < lowe_ratio * second_best_match.distance:\n",
    "        good_matches.append(best_match)\n",
    "        \n",
    "knn_matches_image = cv2.drawMatchesKnn(greg_image, greg_keypoints, memesfunny_image, memesfunny_keypoints, [[match] for match in good_matches], outImg=np.array([]), flags=2)\n",
    "display_image(knn_matches_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7\n",
    "Play around with `lowe_ratio` value.\n",
    "\n",
    "How those changes influence number of found good matches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change `lowe_ratio` to 0.75 again and rerun previous cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display found object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greg_points = np.float32(\n",
    "    [greg_keypoints[match.queryIdx].pt for match in good_matches]\n",
    ").reshape(-1,1,2)\n",
    "memesfunny_points = np.float32(\n",
    "    [memesfunny_keypoints[match.trainIdx].pt for match in good_matches]\n",
    ").reshape(-1,1,2)\n",
    "\n",
    "transformation_matrix, matches_mask = cv2.findHomography(\n",
    "    greg_points, memesfunny_points, cv2.RANSAC, 5.0\n",
    ")\n",
    "matches_mask = matches_mask.ravel().tolist()\n",
    "\n",
    "\n",
    "height, width, *_ = greg_image.shape\n",
    "points = np.float32([\n",
    "    [0, 0],\n",
    "    [0, height - 1],\n",
    "    [width - 1, height - 1],\n",
    "    [width - 1, 0],\n",
    "]).reshape(-1, 1, 2)\n",
    "transformed_points = cv2.perspectiveTransform(\n",
    "    points, transformation_matrix\n",
    ")\n",
    "\n",
    "homography_image = cv2.polylines(\n",
    "    memesfunny_image,\n",
    "    [np.int32(transformed_points)],\n",
    "    True,\n",
    "    (0, 255, 0), # draw border in green color\n",
    "    3,\n",
    "    cv2.LINE_AA,\n",
    ")\n",
    "display_image(homography_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8\n",
    "Change params of polylines function above to display border of matched area in red.\n",
    "\n",
    "**Remember that colors in OpenCV are represented in *BGR* color space**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_params = dict(\n",
    "    matchColor = (0, 255, 0),\n",
    "    singlePointColor = None,\n",
    "    matchesMask = matches_mask,\n",
    "    flags = 2,\n",
    ")\n",
    "\n",
    "homography_image_with_matches = cv2.drawMatches(\n",
    "    greg_image,\n",
    "    greg_keypoints,\n",
    "    homography_image,\n",
    "    memesfunny_keypoints,\n",
    "    good_matches,\n",
    "    None,\n",
    "    **draw_params,\n",
    ")\n",
    "display_image(homography_image_with_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 9\n",
    "Change value of `matchesMask` parameter to `[number % 10 == 0 for number in range(len(matches_mask))]`.\n",
    "\n",
    "What changed? How do you think, what is the reason to use mask for matches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flann Based Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLANN_INDEX_KDTREE = 0\n",
    "KNN_MATCH_NEAREST_NEIGHBOURS_NUMBER = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flann = cv2.FlannBasedMatcher(index_params, search_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Images Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift, estimate_bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image = cv2.imread(LOGO_IMAGE_PATH)\n",
    "target_image = cv2.imread(LOGO_MULTIPLE_SCREENSHOT_IMAGE_PATH)\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(source_image, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(target_image, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints2 = np.array(keypoints2)\n",
    "keypoints2_coordinates = np.array([keypoint.pt for keypoint in keypoints2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANDWIDTH_QUANTILE = 0.1\n",
    "NUMBER_OF_SAMPLES = 500\n",
    "\n",
    "bandwidth = estimate_bandwidth(\n",
    "    keypoints2_coordinates, quantile=BANDWIDTH_QUANTILE, n_samples=NUMBER_OF_SAMPLES\n",
    ")\n",
    "mean_shift = MeanShift(bandwidth=bandwidth, bin_seeding=True, cluster_all=True)\n",
    "mean_shift.fit(keypoints2_coordinates)\n",
    "mean_shift_labels = mean_shift.labels_\n",
    "clusters_labels = np.unique(mean_shift_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_matches = []\n",
    "for cluster_label in clusters_labels:\n",
    "    cluster_points_indices, = np.where(mean_shift_labels == cluster_label)\n",
    "    keypoints2_cluster = keypoints2[cluster_points_indices]\n",
    "    descriptors2_cluster = descriptors2[cluster_points_indices]\n",
    "\n",
    "    matches = flann.knnMatch(\n",
    "        descriptors1, descriptors2_cluster, k=KNN_MATCH_NEAREST_NEIGHBOURS_NUMBER\n",
    "    )\n",
    "    good_matches = []\n",
    "    lowe_ratio = 0.75\n",
    "    for best_match, second_best_match in matches:\n",
    "        if best_match.distance < lowe_ratio * second_best_match.distance:\n",
    "            good_matches.append(best_match)\n",
    "\n",
    "    cluster_matches.append(((keypoints1, keypoints2_cluster), good_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_MATCH_COUNT = 10\n",
    "output_image = target_image\n",
    "for (cluster_keypoints1, cluster_keypoints2), matches in cluster_matches:\n",
    "    if len(matches) >= MIN_MATCH_COUNT: \n",
    "        points1 = np.float32(\n",
    "            [cluster_keypoints1[match.queryIdx].pt for match in matches]\n",
    "        ).reshape(-1,1,2)\n",
    "        points2 = np.float32(\n",
    "            [cluster_keypoints2[match.trainIdx].pt for match in matches]\n",
    "        ).reshape(-1,1,2)\n",
    "\n",
    "        transformation_matrix, matches_mask = cv2.findHomography(\n",
    "            points1, points2, cv2.RANSAC, 5.0\n",
    "        )\n",
    "        if transformation_matrix is None:\n",
    "            continue\n",
    "        matches_mask = matches_mask.ravel().tolist()\n",
    "\n",
    "        height, width, *_ = source_image.shape\n",
    "        points = np.float32([\n",
    "            [0, 0],\n",
    "            [0, height - 1],\n",
    "            [width - 1, height - 1],\n",
    "            [width - 1, 0],\n",
    "        ]).reshape(-1, 1, 2)\n",
    "        transformed_points = cv2.perspectiveTransform(\n",
    "            points, transformation_matrix\n",
    "        )\n",
    "\n",
    "        output_image = cv2.polylines(output_image, [np.int32(transformed_points)], True, 255, 3, cv2.LINE_AA)\n",
    "display_image(output_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 10\n",
    "Play around with `BANDWIDTH_QUANTILE` constant.\n",
    "\n",
    "Try to match as many lion logos as possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
