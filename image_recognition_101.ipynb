{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_recognition_101.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "jT5Os_hDInw2",
        "BBdP9yUaInw7"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "mPT5OcwAInv5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Image recognition 101"
      ]
    },
    {
      "metadata": {
        "id": "ERS8FpZAInv6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Working with images"
      ]
    },
    {
      "metadata": {
        "id": "CnbEmODaeYoA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git clone http://github.com/phankiewicz/image_recognition_101.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PzkyNmiiInv7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -r image_recognition_101/requirements.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JsD5u_zAInv9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import IPython\n",
        "import numpy as np\n",
        "import os\n",
        "import plotly\n",
        "import plotly.graph_objs as go\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "sift = cv2.xfeatures2d.SIFT_create()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MRloTwGiLbH1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "INPUT_BASE_PATH = os.path.join('image_recognition_101', 'input')\n",
        "MEDIA_BASE_PATH = os.path.join('image_recognition_101', 'media')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kxkWf69TInwA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "KEYPOINTS_IMAGE_PATH = os.path.join(MEDIA_BASE_PATH, 'keypoints.jpg')\n",
        "SCALING_IMAGE_PATH = os.path.join(MEDIA_BASE_PATH, 'scaling.jpg')\n",
        "GREG_IMAGE_PATH = os.path.join(INPUT_BASE_PATH, 'greg.png')\n",
        "SUDOKU_IMAGE_PATH = os.path.join(INPUT_BASE_PATH, 'sudoku.jpg')\n",
        "MEMESFUNNY_IMAGE_PATH = os.path.join(INPUT_BASE_PATH, 'memesfunny.png')\n",
        "LOGO_IMAGE_PATH = os.path.join(INPUT_BASE_PATH, 'logo.png')\n",
        "LOGO_MULTIPLE_SCREENSHOT_IMAGE_PATH = os.path.join(INPUT_BASE_PATH, 'logo_multiple_screenshot.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UpTz_zf8AM4v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def configure_plotly_browser_state():\n",
        "  display(\n",
        "      IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "        <script>\n",
        "          requirejs.config({\n",
        "            paths: {\n",
        "              base: '/static/base',\n",
        "              plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',\n",
        "            },\n",
        "          });\n",
        "        </script>\n",
        "        '''\n",
        "      )\n",
        "  )\n",
        "\n",
        "\n",
        "def display_plotly_image(image):\n",
        "  configure_plotly_browser_state()\n",
        "  plotly.offline.init_notebook_mode(connected=False)\n",
        "\n",
        "  img_height, img_width, *_ = image.shape\n",
        "  pil_im = Image.fromarray(image)\n",
        "  if pil_im.mode != 'RGB':\n",
        "    pil_im = pil_im.convert('RGB')\n",
        "\n",
        "\n",
        "  scale_factor = 400/img_width\n",
        "\n",
        "  layout = go.Layout(\n",
        "      xaxis = go.layout.XAxis(\n",
        "          visible = False,\n",
        "          range = [0, img_width*scale_factor]),\n",
        "      yaxis = go.layout.YAxis(\n",
        "          visible=False,\n",
        "          range = [0, img_height*scale_factor],\n",
        "          scaleanchor = 'x'),\n",
        "      width = img_width*scale_factor,\n",
        "      height = img_height*scale_factor,\n",
        "      margin = {'l': 0, 'r': 0, 't': 0, 'b': 0},\n",
        "      images = [go.layout.Image(\n",
        "          x=0,\n",
        "          sizex=img_width*scale_factor,\n",
        "          y=img_height*scale_factor,\n",
        "          sizey=img_height*scale_factor,\n",
        "          xref=\"x\",\n",
        "          yref=\"y\",\n",
        "          opacity=1.0,\n",
        "          layer=\"below\",\n",
        "          sizing=\"stretch\",\n",
        "          source=pil_im)]\n",
        "  )\n",
        "  fig = go.Figure(data=[{\n",
        "      'x': [0, img_width*scale_factor], \n",
        "      'y': [0, img_height*scale_factor], \n",
        "      'mode': 'markers',\n",
        "      'marker': {'opacity': 0}}],layout = layout)\n",
        "  plotly.offline.iplot(fig)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VwH5HQ6XInwC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def display_image(image):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    display_plotly_image(image)\n",
        "    \n",
        "    \n",
        "def display_greyscale_image(image):\n",
        "    display_plotly_image(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ciHXYwhdInwD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "color_image = cv2.imread(GREG_IMAGE_PATH, cv2.IMREAD_COLOR)\n",
        "display_image(color_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1sHytmvoInwP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Edge detection"
      ]
    },
    {
      "metadata": {
        "id": "SdXksxFXInwP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image = cv2.imread(GREG_IMAGE_PATH, cv2.IMREAD_GRAYSCALE)\n",
        "laplacian = cv2.Laplacian(image, cv2.CV_64F)\n",
        "enhanced_laplacian = laplacian.copy()\n",
        "enhanced_laplacian[enhanced_laplacian > 10] = 255\n",
        "display_greyscale_image(enhanced_laplacian)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d_sWCqvTInwS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1\n",
        "In the next cell read image of the sudoku puzzle. Try to detect edges using Laplacian method.\n",
        "\n",
        "Compare it with Sobel operator using following function:\n",
        "\n",
        "`cv2.Sobel(input_image, cv2.CV_64F, 1, 0, ksize=ksize_parameter)`\n",
        "\n",
        "Play around with `ksize` parameter for better results.\n",
        "\n",
        "**Remember that `ksize` parameter can only be odd number from range -1 to 31**"
      ]
    },
    {
      "metadata": {
        "id": "9cPSdOpzInwT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sudoku_image = cv2.imread(SUDOKU_IMAGE_PATH, cv2.IMREAD_GRAYSCALE)\n",
        "# your code goes here\n",
        "display_greyscale_image(edge_detection_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EqxN0qYtInwU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature detection"
      ]
    },
    {
      "metadata": {
        "id": "F69AYCUWzUx5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Keypoints"
      ]
    },
    {
      "metadata": {
        "id": "yg_p4G8XzcLZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "keypoints_image = cv2.imread(KEYPOINTS_IMAGE_PATH, cv2.IMREAD_COLOR)\n",
        "display_image(keypoints_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o4lzu90qBPn0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Rotation and scaling"
      ]
    },
    {
      "metadata": {
        "id": "Bw1puVAwBTYA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scaling_image = cv2.imread(SCALING_IMAGE_PATH, cv2.IMREAD_COLOR)\n",
        "display_image(scaling_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "STIo8CtTInwV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Keypoints detection using SIFT algorithm"
      ]
    },
    {
      "metadata": {
        "id": "LzojPvCkInwW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image = cv2.imread(GREG_IMAGE_PATH, cv2.IMREAD_COLOR)\n",
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "keypoints = sift.detect(image, None)\n",
        "image_with_keypoints = cv2.drawKeypoints(image, keypoints, outImage=np.array([]), flags=cv2.DRAW_MATCHES_FLAGS_DEFAULT)\n",
        "display_image(image_with_keypoints)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qxRduaycInwX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Keypoints detection using ORB algorithm"
      ]
    },
    {
      "metadata": {
        "id": "eGVIcQj4InwY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "orb = cv2.ORB_create()\n",
        "keypoints = orb.detect(image, None)\n",
        "image_with_keypoints = cv2.drawKeypoints(image, keypoints, outImage=np.array([]), flags=cv2.DRAW_MATCHES_FLAGS_DEFAULT)\n",
        "display_image(image_with_keypoints)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nB3amMb3Inwa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Exercise 2\n",
        "\n",
        "Flags for drawKeypoints:\n",
        "+ DRAW_MATCHES_FLAGS_DEFAULT *only center point*\n",
        "+ DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS *center point with keypoint size and orientation*\n",
        "+ DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS *single keypoints will not be drawn*\n",
        "\n",
        "Observe changes in result image using different flags for drawKeypoints function. Use next cell to display keypoints in different way.\n",
        "\n",
        "Try to analyze what are the characteristic features of keypoints that were selected by algorithm."
      ]
    },
    {
      "metadata": {
        "id": "mnLCjI-XInwa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# your code goes here\n",
        "display_image(image_with_keypoints_with_different_flags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HdnRiOX_Inwc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature matching"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "nfdxbgpXInwd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "greg_image = cv2.imread(GREG_IMAGE_PATH)\n",
        "memesfunny_image = cv2.imread(MEMESFUNNY_IMAGE_PATH)\n",
        "\n",
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "greg_keypoints, greg_descriptors = sift.detectAndCompute(greg_image, None)\n",
        "memesfunny_keypoints, memesfunny_descriptors = sift.detectAndCompute(memesfunny_image, None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q-E3hfv3Inwf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Exercise 3\n",
        "Display keypoints found for memesfunny image using `drawKeypoints` function."
      ]
    },
    {
      "metadata": {
        "id": "W1cmA019Inwf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# your code goes here\n",
        "display_image(memesfunny_image_with_keypoints)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZFCkoLs6Inwh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Match"
      ]
    },
    {
      "metadata": {
        "id": "faYwuIs6Inwh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bf = cv2.BFMatcher()\n",
        "matches = bf.match(greg_descriptors, memesfunny_descriptors)\n",
        "\n",
        "first_match = matches[0]\n",
        "\n",
        "print(first_match.distance)\n",
        "print(first_match.queryIdx)\n",
        "print(first_match.trainIdx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5V7ai0k3Inwj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Displaying matches"
      ]
    },
    {
      "metadata": {
        "id": "ny9HsuALInwk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "matches = sorted(matches, key = lambda x:x.distance)\n",
        "matches_image = cv2.drawMatches(greg_image, greg_keypoints, memesfunny_image, memesfunny_keypoints, matches[:10], outImg=np.array([]), flags=2)\n",
        "display_image(matches_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hYLIdCVbInwn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Knn matches"
      ]
    },
    {
      "metadata": {
        "id": "VibM5r73Inwo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "matches = bf.knnMatch(greg_descriptors, memesfunny_descriptors, k=2)\n",
        "match, nearest_neighbour_match = matches[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6qsdgzcdInwq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Exercise 4\n",
        "In the next cell display distance attribute for both `match` and `nearest_neighbour_match`.\n",
        "\n",
        "Compare those values. What do they tell us about specific match?"
      ]
    },
    {
      "metadata": {
        "id": "l8R4QWtIInwq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# your code goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iBekC-ioInwt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Lowe's ratio"
      ]
    },
    {
      "metadata": {
        "id": "ypMg7Ol1Inwt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lowe_ratio = 0.8\n",
        "\n",
        "good_matches = []\n",
        "for best_match, second_best_match in matches:\n",
        "    if best_match.distance < lowe_ratio * second_best_match.distance:\n",
        "        good_matches.append(best_match)\n",
        "        \n",
        "knn_matches_image = cv2.drawMatchesKnn(greg_image, greg_keypoints, memesfunny_image, memesfunny_keypoints, [[match] for match in good_matches], outImg=np.array([]), flags=2)\n",
        "display_image(knn_matches_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OwDuZp1oInwx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Exercise 5\n",
        "Play around with `lowe_ratio` value in the previous cell.\n",
        "\n",
        "How those changes influence number of found good matches?"
      ]
    },
    {
      "metadata": {
        "id": "Re--2gV-Inwx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Homography"
      ]
    },
    {
      "metadata": {
        "id": "_a8Pt8MSInwy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Change `lowe_ratio` to 0.75 again and rerun previous cell\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "GUN0pHPjInwz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Display found object"
      ]
    },
    {
      "metadata": {
        "id": "8ezE0pjvInw0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "greg_points = np.float32(\n",
        "    [greg_keypoints[match.queryIdx].pt for match in good_matches]\n",
        ").reshape(-1,1,2)\n",
        "memesfunny_points = np.float32(\n",
        "    [memesfunny_keypoints[match.trainIdx].pt for match in good_matches]\n",
        ").reshape(-1,1,2)\n",
        "\n",
        "transformation_matrix, matches_mask = cv2.findHomography(\n",
        "    greg_points, memesfunny_points, cv2.RANSAC, 5.0\n",
        ")\n",
        "matches_mask = matches_mask.ravel().tolist()\n",
        "\n",
        "\n",
        "height, width, *_ = greg_image.shape\n",
        "points = np.float32([\n",
        "    [0, 0],\n",
        "    [0, height - 1],\n",
        "    [width - 1, height - 1],\n",
        "    [width - 1, 0],\n",
        "]).reshape(-1, 1, 2)\n",
        "transformed_points = cv2.perspectiveTransform(\n",
        "    points, transformation_matrix\n",
        ")\n",
        "\n",
        "homography_image = cv2.polylines(\n",
        "    memesfunny_image,\n",
        "    [np.int32(transformed_points)],\n",
        "    True,\n",
        "    (0, 255, 0), # draw border in green color\n",
        "    3,\n",
        "    cv2.LINE_AA,\n",
        ")\n",
        "display_image(homography_image)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jT5Os_hDInw2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Exercise 6\n",
        "In the previous cell, change params of polylines function above to display border of matched area in red.\n",
        "\n",
        "**Remember that colors in OpenCV are represented in *BGR* color space**"
      ]
    },
    {
      "metadata": {
        "id": "g93pEK8YInw3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Display matches"
      ]
    },
    {
      "metadata": {
        "id": "UlW6MqAIInw5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "draw_params = dict(\n",
        "    matchColor = (0, 255, 0),\n",
        "    singlePointColor = None,\n",
        "    matchesMask = matches_mask,\n",
        "    flags = 2,\n",
        ")\n",
        "\n",
        "homography_image_with_matches = cv2.drawMatches(\n",
        "    greg_image,\n",
        "    greg_keypoints,\n",
        "    homography_image,\n",
        "    memesfunny_keypoints,\n",
        "    good_matches,\n",
        "    None,\n",
        "    **draw_params,\n",
        ")\n",
        "display_image(homography_image_with_matches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BBdP9yUaInw7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Exercise 7\n",
        "In the previous cell change value of `matchesMask` parameter to `[number % 10 == 0 for number in range(len(matches_mask))]`.\n",
        "\n",
        "What changed? How do you think, what is the reason to use mask for matches?"
      ]
    },
    {
      "metadata": {
        "id": "U7QuqWt4Inw8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Flann Based Matcher"
      ]
    },
    {
      "metadata": {
        "id": "sGnJEjOgInw8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "FLANN_INDEX_KDTREE = 0\n",
        "KNN_MATCH_NEAREST_NEIGHBOURS_NUMBER = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "idJ4zFDTInw-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
        "search_params = dict(checks=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "39FiE5NiInxA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "flann = cv2.FlannBasedMatcher(index_params, search_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CBizizihInxC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Multiple Images Matching"
      ]
    },
    {
      "metadata": {
        "id": "5OjQDMcQInxD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import MeanShift, estimate_bandwidth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2u8tV4WXInxF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "source_image = cv2.imread(LOGO_IMAGE_PATH)\n",
        "target_image = cv2.imread(LOGO_MULTIPLE_SCREENSHOT_IMAGE_PATH)\n",
        "keypoints1, descriptors1 = sift.detectAndCompute(source_image, None)\n",
        "keypoints2, descriptors2 = sift.detectAndCompute(target_image, None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tVLm04UMInxH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "keypoints2 = np.array(keypoints2)\n",
        "keypoints2_coordinates = np.array([keypoint.pt for keypoint in keypoints2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f3MK3aYtInxI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Clustering"
      ]
    },
    {
      "metadata": {
        "id": "PfUg6N6rInxJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BANDWIDTH_QUANTILE = 0.1\n",
        "NUMBER_OF_SAMPLES = 500\n",
        "\n",
        "bandwidth = estimate_bandwidth(\n",
        "    keypoints2_coordinates, quantile=BANDWIDTH_QUANTILE, n_samples=NUMBER_OF_SAMPLES\n",
        ")\n",
        "mean_shift = MeanShift(bandwidth=bandwidth, bin_seeding=True, cluster_all=True)\n",
        "mean_shift.fit(keypoints2_coordinates)\n",
        "mean_shift_labels = mean_shift.labels_\n",
        "clusters_labels = np.unique(mean_shift_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CHUdaeWGInxK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cluster_matches = []\n",
        "for cluster_label in clusters_labels:\n",
        "    cluster_points_indices, = np.where(mean_shift_labels == cluster_label)\n",
        "    keypoints2_cluster = keypoints2[cluster_points_indices]\n",
        "    descriptors2_cluster = descriptors2[cluster_points_indices]\n",
        "\n",
        "    matches = flann.knnMatch(\n",
        "        descriptors1, descriptors2_cluster, k=KNN_MATCH_NEAREST_NEIGHBOURS_NUMBER\n",
        "    )\n",
        "    good_matches = []\n",
        "    lowe_ratio = 0.75\n",
        "    for best_match, second_best_match in matches:\n",
        "        if best_match.distance < lowe_ratio * second_best_match.distance:\n",
        "            good_matches.append(best_match)\n",
        "\n",
        "    cluster_matches.append(((keypoints1, keypoints2_cluster), good_matches))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m36OY7muInxL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MIN_MATCH_COUNT = 10\n",
        "output_image = target_image\n",
        "for (cluster_keypoints1, cluster_keypoints2), matches in cluster_matches:\n",
        "    if len(matches) >= MIN_MATCH_COUNT: \n",
        "        points1 = np.float32(\n",
        "            [cluster_keypoints1[match.queryIdx].pt for match in matches]\n",
        "        ).reshape(-1,1,2)\n",
        "        points2 = np.float32(\n",
        "            [cluster_keypoints2[match.trainIdx].pt for match in matches]\n",
        "        ).reshape(-1,1,2)\n",
        "\n",
        "        transformation_matrix, matches_mask = cv2.findHomography(\n",
        "            points1, points2, cv2.RANSAC, 5.0\n",
        "        )\n",
        "        if transformation_matrix is None:\n",
        "            continue\n",
        "        matches_mask = matches_mask.ravel().tolist()\n",
        "\n",
        "        height, width, *_ = source_image.shape\n",
        "        points = np.float32([\n",
        "            [0, 0],\n",
        "            [0, height - 1],\n",
        "            [width - 1, height - 1],\n",
        "            [width - 1, 0],\n",
        "        ]).reshape(-1, 1, 2)\n",
        "        transformed_points = cv2.perspectiveTransform(\n",
        "            points, transformation_matrix\n",
        "        )\n",
        "\n",
        "        output_image = cv2.polylines(output_image, [np.int32(transformed_points)], True, 255, 3, cv2.LINE_AA)\n",
        "display_image(output_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zau67Pj7InxN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Exercise 8\n",
        "Play around with `BANDWIDTH_QUANTILE` constant define in the first cell of `Clustering` section.\n",
        "\n",
        "Try to match as many lion logos as possible."
      ]
    }
  ]
}